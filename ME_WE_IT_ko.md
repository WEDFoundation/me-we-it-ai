# Me-We-It 책임감 있는 AI 공개 표준

## 책임감 있는 AI 공개 표준이란?

AI가 가져올 수 있는 가치와 피해를 경험한 실무 개발자, 사용자, 이해관계자들로 구성된 500명 이상의 리뷰어와 실무자, 60개국 이상에서 참여하여 AI를 책임감있게 만드는 단계를 투명하게 공개하고 명확히 하기 위해 고안된 공개 제안입니다.

## ‘책임감 있는 AI 공개 표준’의 목적은?

오늘날 우리는 정직한 토론 또는 의사결정을 돕는 명확하고 책임감 있는 소통에 초점을 맞추기보다 히스테리를 부추기고 AI의 구조를 모호하게 만드는 종말론적 서사를 만들어 내고 있기 때문에 아래와 같은 해결책이 필요합니다.

AI 관련 실무자 또는 AI 관련 지식이 없어도:

1. 누구나 이해할 수 있는 윤리적인 AI 개발 방법 소개하기 위함.
2. 누구나 윤리적인 AI 개발과 관련하여 자유롭게 질문할 수 있는 공간 마련하기 위함.
3. 누구나 AI 학습 방식과 생성된 결과물을 모니터링하는 프레임워크에 참여할 수 있도록 하기 위함.

## ‘책임감 있는 AI 공개 표준’ 왜 필요한가요?

AI의 발전은 효율성 증대, 의사 결정 개선 등 이전에는 해결이 불가능했던 문제 해결 능력 등 이점도 있는 반면에 사람들에게 불이익을 주거나, 데이터 프라이버시 및 안전 문제를 증폭시키고, 지적 재산을 도용하고, 편견을 지속시킬 수 있어요. 신중한 고려와 계획 없이 AI를 개발하면 문제를 악화시키는 AI 시스템을 만들 위험이 있어요. 그래서 공개적인 프레임워크를 제안하여 여러분의 질문과 피드백을 수집하여 AI 개발 과정의 개선을 이루어나가는 것을 목표로 합니다. 책임있는 AI 개발을 위한 공개 프레임워크는 아래 링크를 통해서 확인해주세요.

## 책임있는 AI 개발을 위한 공개 프레임워크 버전 1 소개

아래 프레임워크는 여러분의 피드백으로 반복적으로 의미 있는 개선이 이루는 것을 목표로 합니다.

정책 입안자들이 규제를 따라잡기 위해 정신없이 경쟁하고, EU AI 법이 GPAI(범용 AI)를 고위험 범주에 추가할 수 있다는 신호를 보냄에 따라, 모든 팀이 즉시 사용할 수 있는 실행 가능한 표준을 설정하여 AI 개발 방식을 명확히 공개하여 모두가 함께 논의하는 것이 목적입니다. 그리고 해당 제안은 공개 서한이 아닌, 숙련된 기술자, 데이터 과학자, 연구자 및 AI의 영향에 관심이 있는 사람들로 구성된 그룹이 제안하는 공개 제안입니다.

이 공개 제안은 무료 온라인 포럼으로 운영되며, 의견 등록이라는 중요한 가치 아래 새로운 제안과 접근 방식을 초대합니다. AI 구축의 핵심 요소( 학습, 개발, 테스트)와 이 과정에 참여하는 주체를 기준으로 3 단계로 나눕니다.

* **나** (Me): AI 작업을 하는 각 개인이 시작하기 전과 프로세스를 진행하면서 스스로에게 던져야 할 질문입니다.
* **우리** (We): 그룹이 스스로에게 던져야 할 질문으로, 인간의 편견을 최대한 줄이는 데 필요한 다양성을 정의합니다.
* **그것** (It): 만들어지는 모델과 그것이 우리 세계에 미칠 수 있는 영향과 관련하여 개인과 그룹이 스스로에게 던져야 할 질문입니다.

### 1단계. 학습 : 데이터 선택 및 수집

**나**(Me): 1단계를 시작하기 전에 스스로에게 물어봐야 할 필수 질문입니다. 답변은 저장하고 지속적으로 사고 과정이 어떻게 발전하는지 되돌아볼 수 있도록 해야 합니다.

* 해당 학습 데이터를 선택한 이유는 무엇이며, 그러한 선택이 해당 모델에 대한 나의 의도와 어떻게 일치하는가?
* 해당 학습 데이터의 출처는 어디이며, 학습 데이터에 개인 식별 정보(PII), 결제 카드 업계(PCI) 데이터, 보호 대상 건강 정보(PHI) 등 보호되거나 저작권이 있는 자료가 있는가, 데이터 소스 관리를 위해 GDPR, CCPA 또는 기타 시스템을 고려했나요?
* 과거에 AI 또는 머신 러닝 모델에 유사한 데이터를 사용한 경험이 있나요, 아니면 해당 데이터 소스를 처음 사용하는 건가요?
* 이전에 해당 데이터를 사용한 적이 있는 경우, 이전에 해당데이터로 학습된 모델에서 발생한 문제가 있었나요?
* 해당 데이터를 처음 사용하는 경우, 이 데이터가 모델 출력에 미칠 영향에 대해 어떤 기대를 가지고 계신가요?
* 모델 출력에 어떤 영향을 미칠 것으로 예상하나요?
* 모델 카드를 사용하여 위험을 전달한 적이 있으며 해당문서는 협업 팀과 어떻게 업데이트할 것인가요?
* 데이터가 해당 모델에 어떤 역할을 하길 바라며, 내가 의도하는 결과는 무엇이고, 학습 데이터가 의도한 성과에 어떤 영향을 미칠 것으로 기대하나요?
* 선거권이 없는 인구에 대한 도구 변수(예: 세분화된 지역 수준 데이터)로 작용할 수 있는 기능이 있나요? 있다면 학습 데이터에서 학습할 수 있는 편견을 지속시키지 않도록 충분히 제어할 수 있는가?
* 데이터 과학자가 아닌 사람도 이해할 수 있는 방식으로 데이터의 본질을 파악할 수 있도록 훈련 데이터의 내용을 요약할 수 있습니까?
* 의심스러운 출처의 데이터를 학습하기 위해 서두르거나 압박감을 느끼나요?
* 학습 데이터의 출처를 인용할 수 있습니까?
* 내가 해당 데이터를 선택하는 데 어떤 편견이 작용할 수 있나요?
* 내가 이해하지 못하는 편견을 고려하고 있으며, 데이터를 선택할 때 적용되는 편견을 식별하는 데 도움을 줄 수 있는 더 큰 그룹과 내 논리를 공유하고 있나요?
* 해당 학습 데이터가 해당 모델에 어떤 도움이 될 것이라고 생각하는가요?

**우리**(We): 프로세스의 이 부분을 시작하기 전에 실무 팀으로서 물어봐야 할 질문입니다. 우리(팀)의 사고 과정이 어떻게 발전하는지 되돌아볼 수 있도록 답을 저장해 두어야 합니다.

* 해당 모델을 훈련하려는 팀의 의도는 무엇인가요?
* 학습 데이터 전략과 선택을 구축하는 과정에서 누가 협력했나요?
* 데이터 선택의 편향성을 줄이기 위해 다양한 배경과 경험을 가진 사람들이 학습 데이터를 선택하는 작업을 하고 있나요?
* 학습 데이터를 선택한 팀에 내재된 편향성은 무엇인가요?
* 팀 전체가 훈련 데이터의 출처를 이해하고 있으며, 이를 자신의 언어로 설명할 수 있는가?
* 모델 카드를 사용하여 모델의 위험을 전달하고 있으며, 각 기여자가 더 큰 팀과 공유할 수 있는 자체 문서를 보유하고 있나요?
* 제안 중이거나 이미 시행 중인 EU AI 법 또는 기타 규정을 고려했나요?
* 학습 데이터에 개인 식별 정보(PII), 결제 카드 산업(PCI) 데이터, 보호 대상 건강 정보(PHI) 등 보호 대상 또는 저작권이 있는 자료가 있으며, 데이터 소스 관리를 위한 GDPR, CCPA 또는 기타 시스템을 고려했나요?
* 테스트를 위해 훈련 데이터의 몇 퍼센트를 저장하고 있으며 어떻게 선택하고 있나요?

**그것**(It): 알고리즘 또는 모델 개발을 시작하기 전에 알고리즘 또는 모델에 대해 물어봐야 할 질문입니다. 답변은 저장하여 사고 과정이 어떻게 변하는지 되돌아볼 수 있도록 해야 합니다.

* 해당 데이터 세트의 일부가 향후 어느 시점에 불법적이거나, 신뢰할 수 없거나, 받아들일 수 없는 것으로 밝혀질 경우 사용자에게 보상할 수단이 있는가?
* 신중하고 의도적으로 학습하려면 어떤 데이터가 필요한가?
* 전체 데이터 세트의 출처가 알려져 있고, 설명 가능하며, 모델에 유익한가?
* 데이터에 내재된 편향 가능성은 무엇인가요?
* 개인 식별 정보, 보호 대상 데이터 또는 저작권이 있는 자료가 있는가?
* 데이터 세트의 일부로 인해 향후 법적 문제가 발생할 경우 모델에 대한 책임을 처리할 준비가 되어 있는가?
* 의도하지 않은 잠재적 사용/결과는 무엇인가요?
* 모델에 추가되는 학습 데이터를 통해 증폭될 수 있는 편향 가능성은 무엇인가요?

### 2단계. 개발: 알고리즘 및 모델 생성 또는 선택

**나**(Me): 알고리즘 및 모델 생성 또는 선택을 하기 전에 스스로에게 물어봐야 할 질문입니다. 답변은 저장하여 사고 과정이 어떻게 변하는지 되돌아볼 수 있도록 해야 합니다.

* 해당 모델이 무엇을 하도록 의도하고 있으며, 왜 훈련하는가?
* 강화 학습을 실행하는 경우 해당 모델이 실제 환경에서 어떻게 최적화되며 테스트할 결과의 선택이 편향될 가능성은 없나요?
* 전이 학습을 배포하는 경우, 전이 프로세스에서 발견할 수 있는 편향은 무엇인가요?
* 서로 학습하는 앙상블 모델이나 시스템을 실행하는 경우 새로운 편향이나 잘못된 데이터 수집 관행이 시스템에 유입될 가능성이 있나요?
* 해당 모델이 어떻게 작동할 것으로 생각하며, 내가 기대하는 바람직한 출력의 예는 무엇인가요?
* 목표와 추론에 영향을 미치는 내 인간적 편견은 무엇인가요?
* 해당 모델을 처음부터 작성하지 않았다면, 해당모델은 어디에서 왔으며 처음에 어떻게 학습되었나요?

**우리** (We): 알고리즘 및 모델 생성 또는 선택을 하기 전에 해당 부분을 실무 팀으로서 물어봐야 할 질문입니다. 답변은 저장하여 우리의 사고 과정이 어떻게 발전해 왔는지 되돌아볼 수 있도록 해야 합니다.

* 해당 모델을 훈련하고 전략을 구축하는 과정에서 누구와 협업했나요?
* 해당 그룹에는 어떤 인간적 편견이 있으며, 다양한 관점을 포착할 수 있을 만큼 워킹 그룹이 충분히 다양성을 가지고 있는지 고려했나요?
* 모델과 전략을 구축하는 공동 작업자는 누구였나요?
* 이해관계자는 누구이며 모든 이해관계자가 해당 단계의 프로세스에 참여하고 있나요?
* 제안 중이거나 이미 시행 중인 EU AI 법 또는 기타 규정을 고려했나요?
* 개인 식별 정보(PII), 결제 카드 산업(PCI) 데이터, 보호 대상 건강 정보(PHI) 등 보호 대상 또는 저작권이 있는 자료에 대해 모델을 학습시켰으며, 데이터 소스 관리를 위한 GDPR, CCPA 또는 기타 시스템을 고려했나요?

**그것** (It): 알고리즘 및 모델 생성 또는 선택을 하기 전에 알고리즘이나 모델에 대해 스스로에게 물어봐야 할 질문입니다. 답변은 저장하여 사고 프로세스가 어떻게 발전하는지 되돌아볼 수 있도록 해야 합니다.

* 모델의 출처는 어디인가, 아니면 처음부터 개발한 것인가?
* 학습이 완료된 모델의 용도는 무엇인가요?
* 모델을 직접 만들지 않았다면 모델을 처음 만든 사람의 의도를 이해하고 있나요?
* 후속 레벨 결과를 포함하여 의도하지 않은 잠재적 사용/결과는 무엇인가요?
* 모델을 통해 증폭될 수 있는 편견은 무엇인가요?
* 이전에 유사한 데이터를 사용하여 이러한 모델을 배포한 사례가 있으며 의도한 결과와 의도하지 않은 결과는 무엇이었는가?
* 모델의 가능한 위험은 무엇이며 최악의 시나리오에 대한 계획이 있는가?
* 예방 조치로 어떤 유형의 조치를 취했나요?
* 누가 모델을 관리하나요?
* 법률 및 규정의 지속적인 준수를 어떻게 모니터링하고 이행할 것인가?
* 편향성은 어떻게 발견하고 해결하나요?
* 모델을 어떻게 종료할 수 있으며 어떤 상황에서 종료해야 하나요?
* 이를 방해할 수 있는 이해관계자나 자금의 현실적인 문제가 있나요?
* 해당 모델의 관리 및 사용을 지시하는 계약 요건은 무엇인가요?

### 3단계. 테스트: 테스트 데이터 관리 및 태깅

**나**(Me): 테스트 데이터 관리 및 태깅 부분을 시작하기 전에 스스로에게 물어봐야 할 질문입니다. 답변은 저장하여 사고 과정이 어떻게 발전하는지 되돌아볼 수 있도록 해야 합니다.

* 테스트에 사용하고 있는 데이터가 모델의 성능을 분석하기에 충분한가?
* 라이브 환경에서의 사용자 테스트를 고려하고 있으며, 사용자 행동 결과가 원치 않는 것으로 판명될 경우 모델을 어떻게 조정할 것인가?
* 더 큰 그룹과 우리의 접근 방식에 대해 이야기하기 전에 이 모델의 결과를 평가하는 가장 좋은 접근 방식은 무엇이라고 생각하나요?
* 팀과 내가 우리의 작업이 미칠 수 있는 영향을 인식하고 있으며, 작업을 시작하기 전에 테스트해야 할 잠재적으로 나쁜 결과를 모두 고려하고 있다고 확신하는가?
* 테스트 전략과 배포를 철저히 수행하여 모델 결과에서 발생할 수 있는 모든 문제를 발견했다고 믿나요?
* 학습 데이터의 결과와 태거의 피드백이 모델을 만들 때의 원래 의도와 일치했나요? 그렇지 않다면 무엇이 달랐고 어떻게 달랐는지 알 수 있나요?

**우리**(We): 테스트 데이터 관리 및 태깅 부분을 시작하기 전에 실무 팀으로서 물어봐야 할 질문입니다. 답변은 저장하여 우리의 사고 과정이 어떻게 발전하는지 되돌아볼 수 있도록 해야 합니다.

* 해당 모델의 성능과 결과를 어떻게 평가할 것인가?
* 테스트 데이터의 출처를 알고 있으며, 방대한 학습 데이터와 비교하여 테스트 데이터가 대표성을 갖기에 적절한가요?
* 사람이 데이터에 태깅할 경우, 그 사람은 누구이며, 인간적인 대우를 받고 있나요?
* 태깅하는 사람이 데이터에 태그를 지정하기 전에 자신의 의견에 영향을 미칠 수 있는 어떤 지침을 받았나요?
* 태깅하는 사람 또는 실무 팀이 데이터에 대해 어떤 질문을 하는가?
* 태깅 전략이 학습 데이터 및 모델 생성 전략과 일치하는가?
* 데이터 태깅 팀에 적절한 수준의 다양성이 있는가?
* 태깅 또는 테스트 프로세스에 영향을 미칠 수 있는 인간의 편견은 무엇인가요?
* 라이브 환경 데이터에서 사용자 테스트가 수집되고 있으며 반복 프로세스에 어떻게 피드백이 제공되나요?
* 사용자 그룹이 미래의 전체 사용자 그룹을 대표할 수 있는 다양한 사용자층으로 구성되어 있는가?
* 편향성이 모델의 프로그래밍 결과일 가능성은 없는가?
* 테스트 또는 태그 지정 데이터 전략을 구축하는 과정에서 누구와 협업했나요?
* 사용자 데이터를 검색하고 인적 로직을 더 큰 그룹과 공유하는 과정에 제품 팀이 참여했나요?
* 모델이 실행되고 테스트된 후 워킹 그룹의 의도가 실현되었나요? 그렇지 않다면 정량화 가능한 데이터로 자세히 설명하세요.

**그것** (It): 테스트 데이터 관리 및 태깅 부분을 시작하기 전에 알고리즘이나 모델에 대해 스스로에게 물어봐야 할 질문입니다. 답변은 저장하여 사고 과정이 어떻게 발전하는지 되돌아볼 수 있도록 해야 합니다.

* 테스트 결과 또는 태그가 지정된 데이터가 모델 출력에 대한 우리의 의도와 일치했나요?
* 정확도를 결정하기 위해 모델을 테스트하는 경우, 우리가 보고 있는 결과가 모델이 수행할 작업의 핵심 의도와 얼마나 일치하는가?
* 이제 테스트 결과 또는 태그가 지정된 데이터를 기반으로 모델을 조정할 것인가요? 그렇다면 1단계부터 프로세스를 시작하세요.
* 강화 학습을 배포하는 경우, 모델 결과가 어떻게 편향을 증폭시키고 사용자의 새로운 데이터에 영향을 미칠 수 있나요?
* 모델 결과가 EU AI 법 또는 제안 중이거나 이미 시행 중인 다른 규정에 위배되는지 고려했나요?
* 모델 출력에 개인 식별 정보(PII), 결제 카드 산업(PCI) 데이터, 보호 대상 건강 정보(PHI) 등 보호 대상 또는 저작권이 있는 자료가 있는지 테스트했으며, 데이터 소스 관리를 위한 GDPR, CCPA 또는 기타 시스템을 고려했나요?
