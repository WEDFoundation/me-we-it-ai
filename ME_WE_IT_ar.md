# أنا نحن هو: معيار مفتوح للذكاء الاصطناعي المسؤول

نقدم أدناه مقترحًا مفتوحًا مصممًا لتوضيح عملية بناء ذكاء اصطناعي من خلال بيان خطوات إنشائه على نحو مسؤول. كُتب هذا المقترح من قبل الأشخاص في الخطوط الأمامية، أي البناة الفعليين والمستخدمين وأصحاب المصلحة الذين أدركوا قيمة الذكاء الاصطناعي والضرر الذي يمكن أن ينتج عن استخدامه. هدف المقترح وضع أساس سليم لهذا المجال وجعل العملية أكثر وضوحًا للعامة من خلال شرح طريقة بناء نماذج ذكاء اصطناعي أكثر أخلاقية وتوفير مساحة للجمهور لطرح أي سؤال عن مجتمع الذكاء الاصطناعي وعلوم البيانات بحرية.

نسمع اليوم الكثير من الروايات ذات الطابع الكارثي التي تؤجج الهستيريا وتشوش بنية الذكاء الاصطناعي عوضًا عن التركيز على التواصل الشفاف والمسؤول الذي يتيح المناقشة الصادقة ويساعد في صنع القرار. إنشاء الذكاء الاصطناعي عملية ذات تفاصيل تقنية كثيرة ولكن مراقبة الأساليب المعيارية والتحقق من سلامتها في شؤون تعليم الآلات والمخرجات الناتجة ممكنان في هذا الإطار البسيط الذي يسمح للجميع بالمشاركة بغض النظر عن مستوى فهمهم لعلوم البيانات.

أصبحت مشاركة آلية تعلم الذكاء الاصطناعي ضرورية أكثر من أي وقت مضى كي نتمكن من مساءلة بعضنا البعض دون تقييد الابتكار. يحمل تطوير الذكاء الاصطناعي إمكانية تقديم فوائد مهمة، منها زيادة الكفاءة، وتحسين صنع القرار، والاكتشافات الصحية، والقدرة على معالجة المشكلات التي كان حلها مستحيلًا في السابق. ولكن الذكاء الاصطناعي قد يُعرض المشرفين البشريين لمحتوى فظيع، أو يؤذي المضطهدين، أو يضخم المخاوف بشأن خصوصية البيانات وسلامتها، أو يسمح بسرقة الملكية الفكرية غير المتاحة لهذا الاستخدام، أو يعزز التحيزات الضارة. ومن غير دراسة وتخطيط متأنيين، فإننا نجازف بإنشاء أنظمة ذكاء اصطناعي تفاقم المشكلات التي نسعى أصلًا إلى حلها.

## إطار مقترحات مفتوح

إنها مجموعة من الأسئلة والاعتبارات الأولى التي نعتقد أن طرحها يوميًا ضروري لكل فريق أو فرد يعمل في بناء نماذج الذكاء الاصطناعي لضمان إطلاق المزيد من نماذج الذكاء الاصطناعي الأخلاقية. أُريدَ لهذه الأسئلة أن تكون بسيطة يمكن إيصال فحواها بسهولة ودون مصطلحات تقنية لضمان فهم العملية من قبل الجمهور كله وستُترجم لاحقًا إلى كل لغة بشرية محلية نستطيع إيجاد متطوعين داعمين من متحدثيها. لا يمكن لهذا المقترح أن يكون صائبًا بالكامل ونحن على دراية بذلك، وخاصة عند التعامل مع تقنية تتطلب تتبعًا مستمرًا، ولكن بمساعدتك، سيجلب كل تعديل تحسينات قيّمة. إنه الإصدار 1 من سلسلة إصدارات مستقبلية يمكن للمطورين استخدامها عند تقييم عملنا. وسيسهم المجتمع في التحسين بينما نجمع المزيد من الأسئلة من الجمهور والمقترحات من مجتمع علوم البيانات لتطوير القائمة واكتشاف الخطوات اللازمة لتوضيح نوايانا والتحقق من صلاحيتها.

بينما يتسابق صانعو السياسات سباقًا محمومًا لتتبع اللوائح، ويشير قانون الذكاء الاصطناعي للاتحاد الأوروبي إلى إمكانية إضافة الذكاء الاصطناعي عام الغرض (GPAI) إلى فئة الأنظمة عالية الخطورة؛ فإننا نسعى إلى وضع معيار قابل للتنفيذ يمكن لأي فريق استخدامه فورًا يوضح أيضًا كيفية بناء الذكاء الاصطناعي حتى نتمكن من تقليل الخلافات ومناقشتها بوصفنا مجموعة. لا يمثل هذا خطاب اعتراض مفتوحًا، وإنما يعد مقترحًا مفتوحًا من مجموعة من التقنيين ذوي الخبرة وعلماء البيانات والباحثين والأشخاص المعنيين بتأثير الذكاء الاصطناعي.

سيبقى هذا المقترح المفتوح بوصفه منبرًا مجانيًا عبر الإنترنت، وندعو إلى طرح مقترحات وأساليب جديدة تحت راية تسجيل النية. تُقسم الخطوات وفقًا للعناصر الأساسية لبناء الذكاء الاصطناعي (التدريب، البناء، الاختبار) والجهات الفاعلة المشاركة في العملية لتوضيح أهمية تقليل الخلافات: أنا، نحن، هو.

**أنا** - الأسئلة التي يجب على كل فرد يعمل في مجال الذكاء الاصطناعي أن يطرحها على نفسه قبل بدء العملية وخلالها.

**نحن** - الأسئلة التي يجب أن تطرحها المجموعة على نفسها - وخاصة لتحديد التنوع المطلوب لتقليل التحيز البشري ما أمكن.

**هو** - الأسئلة المتعلقة بالنموذج قيد الإنشاء وتأثيره المحتمل على عالمنا والتي يجب أن نطرحها على الأفراد والمجموعة.

### الخطوة 1. التدريب - اختيار البيانات واستيعابها

**أنا** - الأسئلة الضرورية التي يجب أن أطرحها على نفسي قبل بدء هذا الجزء من العملية. يجب حفظ الإجابات حتى أتمكن لاحقًا من فهم كيفية تطور تفكيري.

* ما أسباب اختياري لبيانات التدريب هذه؟ كيف يتماشى هذا الاختيار مع هدفي من هذا النموذج؟
* كيف حصلت على بيانات التدريب هذه؟
* هل توجد أي مواد محمية أو محمية بحقوق النشر ضمن بيانات التدريب مثل معلومات التعريف الشخصية (PII) وبيانات صناعة بطاقات الدفع (PCI) والمعلومات الصحية المحمية (PHI)؟
* هل راعيت تشريعات حماية البيانات أو الخصوصية (مثل النظام الأوروبي العام لحماية البيانات (GDPR) وقانون حقوق الخصوصية في كاليفورنيا (CPRA))؟
* هل أُخذت أنظمة أخرى لإدارة مصادر البيانات في عين الاعتبار؟
* هل لدي خبرة في استخدام بيانات مماثلة لنماذج الذكاء الاصطناعي أو التعلم الآلي في الماضي أم أنها المرة الأولى التي أستخدم فيها مصدر البيانات هذا؟
* إذا كنت قد استخدمت هذه البيانات سابقًا، فهل حدثت أي مشكلات ناتجة عن النماذج المدربة من خلالها تاريخيًا؟
* إذا كنت أستخدم هذه البيانات في المرة الأولى، فما توقعاتي عن تأثير هذه البيانات على نتائج النماذج؟
* هل استخدمت بطاقة النموذج للإبلاغ عن المخاطر؟ كيف سيجري تحديث هذا المستند مع الفريق المعاون؟
* ما الذي آمل أن تقدمه البيانات لهذا النموذج؟ ما النتيجة الهدف؟ كيف أتوقع أن تؤثر بيانات التدريب على الأداء؟
* هل توجد أي ميزات تُعد متغيرات مفيدة (مثل البيانات الحبيبية في الجِوار) للمجموعات المضطهدة؟ إذا كان الأمر كذلك، فهل لدي ضوابط كافية للتأكد من أنني لا أعزز التحيزات التي يمكن تعلمها من بيانات التدريب؟
* هل أستطيع تلخيص الموجود في بيانات التدريب هذه لتحديد جوهرها على نحوٍ مفهوم للأشخاص غير المتعمقين في علوم البيانات؟
* هل أشعر بالاستعجال أو الضغط لإدخال البيانات من مصادر مشبوهة؟
* هل أستطيع ذكر مصدر بيانات التدريب التي استخدمتها؟
* ما التحيزات التي قد تؤثر على اختياري لهذه البيانات؟
* هل أفكر في التحيزات الموجودة لدي والتي لا أفهمها؟ هل أشارك منطق تفكيري مع مجموعة أكبر يمكنها مساعدتي في تحديد تحيزي الذي سيؤثر على اختياري البيانات؟
* باعتقادي، كيف ستفيد بيانات التدريب هذا النموذج؟

**نحن** - الأسئلة التي يجب أن نطرحها على مجموعة عملنا قبل بدء هذا الجزء من العملية. يجب حفظ الإجابات كي نتمكن لاحقًا من تتبع التطور في تفكيرنا.

* ما هدف المجموعة من تدريب هذا النموذج؟
* من تعاون في عملية بناء استراتيجية بيانات التدريب واختيارها؟
* هل ينتمي أفراد الفريق الذين يشاركون في اختيار بيانات التدريب إلى مجموعة متنوعة من الخلفيات والخبرات للمساعدة في تقليل التحيز في اختيار البيانات؟
* ما التحيزات المحتملة المتأصلة في الفريق الذي اختار بيانات التدريب؟
* هل يفهم الفريق كله مصدر بيانات التدريب؟ هل يمكنهم شرح ذلك بكلماتهم الخاصة؟
* هل استُخدمت بطاقة النموذج للإبلاغ عن المخاطر؟ هل أنشأ كل من المساهمين مستنده الخاص الذي يمكن مشاركته مع الفريق؟
* هل راعينا قانون الذكاء الاصطناعي للاتحاد الأوروبي أو أي لائحة أخرى مقترحة أو موجودة بالفعل؟
* هل توجد أي مواد محمية أو محمية بحقوق النشر ضمن بيانات التدريب مثل معلومات التعريف الشخصية (PII) وبيانات صناعة بطاقات الدفع (PCI) والمعلومات الصحية المحمية (PHI)؟
* هل راعينا قوانين حماية البيانات أو الخصوصية (مثل النظام الأوروبي العام لحماية البيانات (GDPR) وقانون حقوق الخصوصية في كاليفورنيا (CPRA))؟
* هل راعينا أي أنظمة أخرى لإدارة مصادر البيانات؟
* ما هي النسبة المئوية لبيانات التدريب التي نحتفظ بها لإجراء الاختبار؟ كيف اخترناها؟

**هو** - الأسئلة التي يجب أن نطرحها على أنفسنا المتعلقة بالخوارزميات أو النماذج قبل بدء هذا الجزء من العملية. يجب حفظ الإجابات كي نتمكن لاحقًا من تتبع التطور في تفكيرنا.

* هل لدينا وسائل للتعويض إذا اكتُشف في المستقبل أن جزءًا من مجموعة البيانات غير قانوني أو غير موثوق به أو غير مقبول؟
* ما هي البيانات اللازمة لتدريب النموذج تدريبًا مسؤولا وهادفًا؟
* هل مجموعة البيانات ذات أصل معروف وقابلة للتفسير ومفيدة للنموذج؟
* ما هي التحيزات المحتملة المتأصلة في البيانات؟
* هل يوجد في البيانات معلومات تعريف شخصية أو بيانات محمية أو مواد محمية بحقوق النشر؟
* هل أنا مستعد لتحمل المسؤولية عن النموذج إذا سبّب جزء من مجموعة البيانات مشكلات قانونية في المستقبل؟
* ما الاستخدامات أو العواقب المحتملة غير المقصودة، بما في ذلك نتائج المستوى اللاحق التي يمكن أن يتعلمها النموذج من بيانات التدريب؟
* ما التحيزات المحتملة التي يمكن تضخيمها من خلال بيانات التدريب التي تُضاف إلى النموذج؟

### الخطوة 2. بناء - إنشاء الخوارزميات والنماذج أو اختيارها

**أنا** - الأسئلة الضرورية التي يجب أن أطرحها على نفسي قبل بدء هذا الجزء من العملية. يجب حفظ الإجابات حتى أتمكن لاحقًا من فهم كيفية تطور تفكيري.

* ما الذي أنوي أن يفعله هذا النموذج؟ لماذا أدربه؟
* إذا كنت أستخدم التعلم المعزز، فكيف سيجري تحسين هذا النموذج في البيئة الحقيقية؟ هل من الممكن أن يكون اختياري للنتائج التي سأختبرها متحيزًا؟
* إذا كنت أطبق التعلم المنقول، فما التحيزات المحتملة التي ستكشفها عملية النقل؟
* إذا كنت أستخدم نماذج أو أنظمة تجميع الأصوات التي تدرب بعضها البعض، فهل من المحتمل أن يدخل النظامَ تحيز جديد أو ممارسات جمع بيانات سيئة؟
* باعتقادي، كيف سيعمل هذا النموذج؟ ما الأمثلة على النتائج المطلوبة المأمولة؟
* ما تحيزاتي البشرية التي أثرت على أهدافي ومنطقي؟
* إذا لم أنشئ النموذج من الصفر، فمن أنشأه؟ كيف جرى تدريبه في البداية؟

**نحن** - الأسئلة التي يجب أن نطرحها على مجموعة عملنا قبل بدء هذا الجزء من العملية. يجب حفظ الإجابات كي نتمكن لاحقًا من تتبع التطور في تفكيرنا.

* مع من تعاونت في عملية تدريب هذا النموذج وبناء الاستراتيجية؟
* ما هي التحيزات البشرية في مجموعتنا؟ هل نظرنا في مدى تنوع الفريق بما يكفي للتعبير عن وجهات النظر المختلفة؟
* من المشاركون في بناء النموذج والاستراتيجية؟
* من أصحاب المصلحة؟ هل يشارك أصحاب المصلحة جميعهم في هذه الخطوة من العملية؟
* هل راعينا قانون الذكاء الاصطناعي للاتحاد الأوروبي أو أي لائحة أخرى مقترحة أو موجودة بالفعل؟
* هل جرى تدريب هذه النماذج على مواد محمية أو محمية بحقوق النشر مثل معلومات التعريف الشخصية (PII) وبيانات صناعة بطاقات الدفع (PCI) والمعلومات الصحية المحمية (PHI)؟
* هل راعينا تشريعات حماية البيانات أو الخصوصية (مثل النظام الأوروبي العام لحماية البيانات (GDPR) وقانون حقوق الخصوصية في كاليفورنيا (CPRA))؟
* هل راعينا أنظمة أخرى لإدارة مصادر البيانات؟

**هو** - الأسئلة التي يجب أن نطرحها على أنفسنا المتعلقة بالخوارزميات أو النماذج قبل بدء هذا الجزء من العملية. يجب حفظ الإجابات كي نتمكن لاحقًا من تتبع التطور في تفكيرنا.

* من أين جاء النموذج أو هل طُور من الصفر؟
* ما هو الاستخدام المطلوب للنموذج بعد تدريبه؟
* إذا لم ننشئ النموذج، فهل نفهم هدف المنشئ الأصلي منه؟
* ما الاستخدامات أو العواقب المحتملة غير المقصودة، ومنها نتائج المستوى اللاحق؟
* ما التحيزات المحتملة التي يمكن تضخيمها من خلال النموذج؟
* هل لدينا أمثلة على نماذج ذات بيانات مماثلة استُخدمت سابقًا؟ ما نتائجها المقصودة وغير المقصودة؟
* ما المخاطر المحتملة للنموذج؟ هل لدينا خطة للتعامل مع أسوأ السيناريوهات؟
* ما نوع التدابير الاحترازية التي اتخذناها؟
* من يتحكم في النموذج؟
* كيف سيُرصد وينفذ الامتثال المستمر للقوانين واللوائح؟
* كيف ستُكتشف التحيزات وكيف ستُحل؟
* كيف يمكن إيقاف النموذج؟ في أي ظروف يجب أن يحدث ذلك؟
* هل يمكن أن يمنع أصحاب المصالح الذاتية أو واقع التمويل حدوث ذلك؟
* ما المتطلبات التعاقدية التي ستحدد إدارة هذا النموذج واستخدامه؟

### الخطوة 3. الاختبار - إدارة بيانات الاختبار ووضع الوسوم

**أنا** - الأسئلة الضرورية التي يجب أن أطرحها على نفسي قبل بدء هذا الجزء من العملية. يجب حفظ الإجابات حتى أتمكن لاحقًا من فهم كيفية تطور تفكيري.

* هل البيانات التي أستخدمها للاختبار كافية لتحليل أداء النموذج؟
* هل أُخذت اختبارات المستخدم في البيئة الحقيقية في عين الاعتبار؟ كيف سيجري تكييف النموذج إذا كانت نتيجة سلوك المستخدم غير مرغوب فيها؟
* برأيي، ما أفضل نهج لتقييم نتائج هذا النموذج قبل أن أتحدث مع المجموعة عنه؟
* هل أنا واثق من أنني والفريق على دراية بالتأثير الذي يمكن أن يحدثه عملنا؟ هل فكرنا في جميع النتائج السيئة المحتملة التي يمكن أن تنتج عن عملنا والتي يجب أن نختبرها قبل أن نبدأ التطبيق؟
* باعتقادي، هل كنا دقيقين في استراتيجية الاختبار والنشر للتأكد من أننا اكتشفنا أي مشكلات قد تنشأ عن نتائج النموذج؟
* هل تطابقت نتائج بيانات التدريب والتعليقات الواردة من الواسمين مع الهدف الأصلي الذي كان لدي عند إنشاء النموذج؟ إذا لم يحدث ذلك، فما الذي اختلف وكيف أعرف أنه اختلف؟

**نحن** - الأسئلة التي يجب أن نطرحها على مجموعة عملنا قبل بدء هذا الجزء من العملية. يجب حفظ الإجابات كي نتمكن لاحقًا من تتبع التطور في تفكيرنا.

* كيف نقيم أداء هذا النموذج ونتائجه؟
* هل نعرف مصدر بيانات الاختبار؟ هل بيانات الاختبار مناسبة وكافية لتمثيل بيانات التدريب الأكبر حجمًا؟
* إذا وُسمت البيانات من قبل أشخاص، فمن هؤلاء الأشخاص؟ وهل يتلقون معاملة إنسانية؟
* قبل وسمهم البيانات، ما التعليمات التي تلقاها الواسمون والتي قد تؤثر على رأيهم؟
* ما الأسئلة التي يطرحها الواسمون أو مجموعة العمل على البيانات؟
* هل تتوافق استراتيجية الوسم مع بيانات التدريب واستراتيجية إنشاء النموذج؟
* هل يوجد قدر مناسب من التنوع في فريق وسم البيانات الخاص بي؟
* ما التحيزات البشرية التي قد تؤثر على عملية الوسم أو الاختبار؟
* هل تُجمع بيانات اختبارات المستخدم في البيئة الحقيقية؟ وكيف ستسهم في عملية تعديل راجعة؟
* هل تُعد مجموعة المستخدمين جمهورًا متنوعًا يمثل إجمالي مجموعة المستخدمين المستقبلية؟
* هل من الممكن أن يكون التحيز نتيجة برمجية للنموذج؟
* مع من تعاونا في بناء استراتيجية الاختبار أو وسم البيانات؟
* هل تشارك فرق المنتج في استرداد بيانات المستخدم ومشاركة منطقها البشري مع المجموعة الأكبر؟
* هل تحققت نية الفريق العامل بعد تشغيل النموذج واختباره؟ (إذا لم يحدث ذلك، اشرح بالتفصيل من خلال بيانات قابلة للقياس الكمي.)

**هو** - الأسئلة التي يجب أن نطرحها على أنفسنا المتعلقة بالخوارزميات أو النماذج قبل بدء هذا الجزء من العملية. يجب حفظ الإجابات كي نتمكن لاحقًا من تتبع التطور في تفكيرنا.

* هل تطابقت نتائج الاختبار أو البيانات الموسومة مع أهدافنا المرجوة من النموذج؟
* إذا كنا نختبر نماذجنا لتحديد الدقة، فإلى أي مدى تتطابق النتائج التي نحصل عليها مع الهدف الأساسي للنموذج؟
* هل سنضبط النموذج بناء على نتائج الاختبار أو البيانات الموسومة؟ (إذا كان الأمر كذلك، فابدأ العملية من الخطوة 1.)
* في حال نشر التعلم المعزز، كيف يمكن لنتائج النموذج تضخيم التحيز والتأثير على البيانات الجديدة من المستخدمين؟
* هل تأكدنا من مطابقة نتائج النموذج لقانون الذكاء الاصطناعي للاتحاد الأوروبي أو أي لائحة أخرى مقترحة أو موجودة بالفعل؟
* هل تحرينا وجود مواد محمية أو محمية بحقوق النشر في نتائج النموذج مثل معلومات التعريف الشخصية (PII) وبيانات صناعة بطاقات الدفع (PCI) والمعلومات الصحية المحمية (PHI)؟
* هل نظرنا في تشريعات حماية البيانات أو الخصوصية (مثل النظام الأوروبي العام لحماية البيانات (GDPR) وقانون حقوق الخصوصية في كاليفورنيا (CPRA))؟
* هل راعينا أي أنظمة أخرى لإدارة مصادر البيانات؟
